## 2、Transformer 为何使用多头注意力机制？为什么不使用一个头？

---

Transformer 中使用多头注意力机制（Multi-Head Attention），核心目的是提升模型的表示能力。相比单一注意力头，多头结构能够从多个子空间并行建模不同的注意力关系，捕捉更丰富的语义信息。

### 🎯 表达多样性（Representation Diversity）

单个注意力头只能在一个空间中计算 token 之间的关系，表达能力有限；而多头注意力通过多个线性变换，将 Q、K、V 投影到不同的子空间：

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

每个 Head 学习到的关注模式是不同的：
- 有的 Head 关注短程依赖（如词法搭配）
- 有的 Head 关注长程依赖（如主谓一致、跨句结构）
- 有的 Head 可能自然学习到句法或语义角色

这些不同关注角度最终被拼接再整合：

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\dots,\text{head}_h)W^O
$$

---

### ✅ 总结一句话：

Transformer 使用 **多头注意力机制** 主要是为了使模型捕捉到更丰富的**特征信息**。使用一个头只能将词向量转换到一个空间中进行计算，限制了信息捕捉的多样性。而 **多头机制** 通过多个头并行计算**不同子空间的注意力**，能够同时学习到词语与词语之间的多种关系，提升了模型的表达和泛化能力。此外，多头机制也利用了 **并行计算** 的优势，提升了效率。


